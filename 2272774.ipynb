{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 查看当前挂载的数据集目录, 该目录下的变更重启环境后会自动还原\n",
    "# View dataset directory. \n",
    "# This directory will be recovered automatically after resetting environment. \n",
    "!ls /home/aistudio/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test1.csv  train.csv\r\n"
     ]
    }
   ],
   "source": [
    "# 查看工作区文件, 该目录下的变更将会持久保存. 请及时清理不必要的文件, 避免加载过慢.\n",
    "# View personal work directory. \n",
    "# All changes under this directory will be kept even after reset. \n",
    "# Please clean unnecessary files in time to speed up environment loading. \n",
    "!ls /home/aistudio/work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘/home/aistudio/external-libraries’: File exists\n",
      "Looking in indexes: https://mirror.baidu.com/pypi/simple/\n",
      "Collecting catboost\n",
      "\u001b[?25l  Downloading https://mirror.baidu.com/pypi/packages/f4/f8/1943b42a4db7be411ab6e8bb8f5cf44b33f6f9e83665cc914d864a228d46/catboost-0.26.1-cp37-none-manylinux1_x86_64.whl (67.4MB)\n",
      "\u001b[K     |████████████████████████████████| 67.4MB 8.8MB/s eta 0:00:012\n",
      "\u001b[?25hCollecting pandas>=0.24.0 (from catboost)\n",
      "\u001b[?25l  Downloading https://mirror.baidu.com/pypi/packages/12/01/360d7f444f910ae16496c07e3f003cb8c641b4ca6c033408a4469a904df3/pandas-1.3.1.tar.gz (4.7MB)\n",
      "\u001b[K     |████████████████████████████████| 4.7MB 15.5MB/s eta 0:00:01\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting numpy>=1.16.0 (from catboost)\n",
      "\u001b[?25l  Downloading https://mirror.baidu.com/pypi/packages/f9/d5/18336e9828d2f07beb0bcd3849c660001bedea50e6219627315968900ad6/numpy-1.21.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7MB)\n",
      "\u001b[K     |████████████████████████████████| 15.7MB 5.1MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting plotly (from catboost)\n",
      "\u001b[?25l  Downloading https://mirror.baidu.com/pypi/packages/95/8d/ac1560f7ccc2ace85cd1e9619bbec1975b5d2d92e6c6fdbbdaa994c6ab4d/plotly-5.1.0-py2.py3-none-any.whl (20.6MB)\n",
      "\u001b[K     |████████████████████████████████| 20.6MB 8.2MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting matplotlib (from catboost)\n",
      "\u001b[?25l  Downloading https://mirror.baidu.com/pypi/packages/24/33/5568d443ba438d95d4db635dd69958056f087e57e1026bee56f959d53f9d/matplotlib-3.4.2-cp37-cp37m-manylinux1_x86_64.whl (10.3MB)\n",
      "\u001b[K     |████████████████████████████████| 10.3MB 26.8MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting scipy (from catboost)\n",
      "\u001b[?25l  Downloading https://mirror.baidu.com/pypi/packages/b5/6b/8bc0b61ebf824f8c3979a31368bbe38dd247590049a994ab0ed077cb56dc/scipy-1.7.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (28.5MB)\n",
      "\u001b[K     |████████████████████████████████| 28.5MB 9.4MB/s eta 0:00:012\n",
      "\u001b[?25hCollecting six (from catboost)\n",
      "  Downloading https://mirror.baidu.com/pypi/packages/d9/5a/e7c31adbe875f2abbb91bd84cf2dc52d792b5a01506781dbcf25c91daf11/six-1.16.0-py2.py3-none-any.whl\n",
      "Collecting graphviz (from catboost)\n",
      "  Downloading https://mirror.baidu.com/pypi/packages/97/14/b5eeeb6d24dbca0ada857ce4a453985df34d9512464bb20cc1a8aca44c54/graphviz-0.17-py3-none-any.whl\n",
      "Collecting pytz>=2017.3 (from pandas>=0.24.0->catboost)\n",
      "\u001b[?25l  Downloading https://mirror.baidu.com/pypi/packages/70/94/784178ca5dd892a98f113cdd923372024dc04b8d40abe77ca76b5fb90ca6/pytz-2021.1-py2.py3-none-any.whl (510kB)\n",
      "\u001b[K     |████████████████████████████████| 512kB 24.1MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting python-dateutil>=2.7.3 (from pandas>=0.24.0->catboost)\n",
      "\u001b[?25l  Downloading https://mirror.baidu.com/pypi/packages/36/7a/87837f39d0296e723bb9b62bbb257d0355c7f6128853c78955f57342a56d/python_dateutil-2.8.2-py2.py3-none-any.whl (247kB)\n",
      "\u001b[K     |████████████████████████████████| 256kB 23.1MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tenacity>=6.2.0 (from plotly->catboost)\n",
      "  Downloading https://mirror.baidu.com/pypi/packages/f2/a5/f86bc8d67c979020438c8559cc70cfe3a1643fd160d35e09c9cca6a09189/tenacity-8.0.1-py3-none-any.whl\n",
      "Collecting kiwisolver>=1.0.1 (from matplotlib->catboost)\n",
      "\u001b[?25l  Downloading https://mirror.baidu.com/pypi/packages/d2/46/231de802ade4225b76b96cffe419cf3ce52bbe92e3b092cf12db7d11c207/kiwisolver-1.3.1-cp37-cp37m-manylinux1_x86_64.whl (1.1MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1MB 23.2MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pillow>=6.2.0 (from matplotlib->catboost)\n",
      "\u001b[?25l  Downloading https://mirror.baidu.com/pypi/packages/8e/7a/b047f6f80fdb02c0cca1d3761d71e9800bcf6d4874b71c9e6548ec59e156/Pillow-8.3.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (3.0MB)\n",
      "\u001b[K     |████████████████████████████████| 3.0MB 13.4MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyparsing>=2.2.1 (from matplotlib->catboost)\n",
      "\u001b[?25l  Downloading https://mirror.baidu.com/pypi/packages/8a/bb/488841f56197b13700afd5658fc279a2025a39e22449b7cf29864669b15d/pyparsing-2.4.7-py2.py3-none-any.whl (67kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 15.3MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cycler>=0.10 (from matplotlib->catboost)\n",
      "  Downloading https://mirror.baidu.com/pypi/packages/f7/d2/e07d3ebb2bd7af696440ce7e754c59dd546ffe1bbe732c8ab68b9c834e61/cycler-0.10.0-py2.py3-none-any.whl\n",
      "Building wheels for collected packages: pandas\n",
      "  Building wheel for pandas (PEP 517) ... \u001b[?25l-^C\n",
      "\u001b[?25canceled\n"
     ]
    }
   ],
   "source": [
    "# 如果需要进行持久化安装, 需要使用持久化路径, 如下方代码示例:\n",
    "# If a persistence installation is required, \n",
    "# you need to use the persistence path as the following: \n",
    "!mkdir /home/aistudio/external-libraries\n",
    "!pip install catboost -t /home/aistudio/external-libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 同时添加如下代码, 这样每次环境(kernel)启动的时候只要运行下方代码即可: \n",
    "# Also add the following code, \n",
    "# so that every time the environment (kernel) starts, \n",
    "# just run the following code: \n",
    "import sys \n",
    "sys.path.append('/home/aistudio/external-libraries')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "请点击[此处](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576)查看本环境基本用法.  <br>\n",
    "Please click [here ](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576) for more detailed instructions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys \r\n",
    "sys.path.append('/home/aistudio/external-libraries')\r\n",
    "import lightgbm as lgb\r\n",
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "from sklearn.ensemble import RandomForestRegressor\r\n",
    "from sklearn.ensemble import RandomForestClassifier\r\n",
    "from sklearn.multioutput import MultiOutputRegressor\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "from sklearn import preprocessing\r\n",
    "from sklearn import preprocessing\r\n",
    "from sklearn.model_selection import GroupKFold, KFold, StratifiedKFold\r\n",
    "from catboost import CatBoostClassifier\r\n",
    "import gc\r\n",
    "import re\r\n",
    "import warnings\r\n",
    "\r\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 数据读取特征类型转换\r\n",
    "def data_preprocessing(train, test1):\r\n",
    "    df1 = train.drop(['Unnamed: 0'], axis=1)\r\n",
    "    df2 = test1.drop(['Unnamed: 0'], axis=1)\r\n",
    "    df2[\"label\"] = -1\r\n",
    "    li = []\r\n",
    "    for df_index in [df1,df2]:\r\n",
    "        for col in [\"android_id\", \"apptype\", \"carrier\", \"ntt\", \"media_id\", \"cus_type\", \"package\", 'fea1_hash', \"location\"]:\r\n",
    "            df_index[col] = df_index[col].astype(\"object\")\r\n",
    "        for col in [\"fea_hash\"]:\r\n",
    "            df_index[col] = df_index[col].map(lambda x: 0 if len(str(x)) > 16 else int(x))\r\n",
    "        for col in [\"dev_height\", \"dev_ppi\", \"dev_width\", \"fea_hash\", \"label\"]:\r\n",
    "            df_index[col] = df_index[col].astype(\"int64\")\r\n",
    "        df_index[\"truetime\"] = pd.to_datetime(df_index['timestamp'], unit='ms', origin=pd.Timestamp('1970-01-01'))\r\n",
    "        df_index[\"day\"] = df_index.truetime.dt.day\r\n",
    "        df_index[\"hour\"] = df_index.truetime.dt.hour\r\n",
    "        df_index[\"minute\"] = df_index.truetime.dt.minute\r\n",
    "        df_index.set_index(\"sid\", drop=True, inplace=True)\r\n",
    "        df_index.dev_height[df_index.dev_height == 0] = None\r\n",
    "        df_index.dev_width[df_index.dev_width == 0] = None\r\n",
    "        df_index.dev_ppi[df_index.dev_ppi == 0] = None\r\n",
    "        li.append(df_index)\r\n",
    "    df2[\"label\"] = None\r\n",
    "    return li\r\n",
    "# 类别预处理\r\n",
    "def process_category(df1, df2, col):\r\n",
    "    le = preprocessing.LabelEncoder()\r\n",
    "    df1[col] = le.fit_transform(df1[col])\r\n",
    "    df1[col] = df1[col].astype(\"object\")\r\n",
    "    df2[col] = le.transform(df2[col])\r\n",
    "    df2[col] = df2[col].astype(\"object\")\r\n",
    "    return df1, df2\r\n",
    "\r\n",
    "\r\n",
    "def dict_category(df1, df2, col, dict1):\r\n",
    "    print(col, dict1)\r\n",
    "    df1[col] = df1[col].map(dict1)\r\n",
    "    df1[col] = df1[col].astype(\"object\")\r\n",
    "    df2[col] = df2[col].map(dict1)\r\n",
    "    df2[col] = df2[col].astype(\"object\")\r\n",
    "    return df1, df2\r\n",
    "\r\n",
    "\r\n",
    "def filter_value(df1, df2, col, top, other=-1):\r\n",
    "    set1 = set(df1[col].value_counts().head(top).index)\r\n",
    "    def process_temp(x):\r\n",
    "        if x in set1:\r\n",
    "            return x\r\n",
    "        else:\r\n",
    "            return other\r\n",
    "    df1[col] = df1[col].apply(process_temp)\r\n",
    "    df2[col] = df2[col].apply(process_temp)\r\n",
    "    return df1, df2\r\n",
    "\r\n",
    "\r\n",
    "def special_category(df1, df2, col):\r\n",
    "    if col == \"apptype\":\r\n",
    "        df1, df2 = filter_value(df1, df2, col, 75, -1)\r\n",
    "    if col == \"media_id\":\r\n",
    "        df1, df2 = filter_value(df1, df2, col, 200, -1)\r\n",
    "    if col == \"version\":\r\n",
    "        df2[col] = df2[col].replace(\"20\", \"0\").replace(\"21\", \"0\")\r\n",
    "    if col == \"lan\":\r\n",
    "        def foreign_lan(x):\r\n",
    "            set23 = {'zh-CN', 'zh', 'cn', 'zh_CN', 'Zh-CN', 'zh-cn', 'ZH', 'CN', 'zh_CN_#Hans'}\r\n",
    "            if x in set23:\r\n",
    "                return 0\r\n",
    "            elif x == \"unk\":\r\n",
    "                return 2\r\n",
    "            else:\r\n",
    "                return 1\r\n",
    "        df1[\"vpn\"] = df1[\"lan\"].apply(foreign_lan)\r\n",
    "        df2[\"vpn\"] = df2[\"lan\"].apply(foreign_lan)\r\n",
    "        set12 = {'zh-CN', 'zh', 'cn', 'zh_CN', 'Zh-CN', 'zh-cn', 'ZH', 'CN', 'tw', 'en', 'zh_CN_#Hans', 'ko'}\r\n",
    "        def process_lan(x):\r\n",
    "            if x in set12:\r\n",
    "                return x\r\n",
    "            else:\r\n",
    "                return \"unk\"\r\n",
    "        df1[col] = df1[col].apply(process_lan)\r\n",
    "        df2[col] = df2[col].apply(process_lan)\r\n",
    "    if col == \"package\":\r\n",
    "        df1, df2 = filter_value(df1, df2, col, 800, -1)\r\n",
    "    if col == \"fea1_hash\":\r\n",
    "        df1, df2 = filter_value(df1, df2, col, 850, -1)\r\n",
    "    if col == \"fea_hash\":\r\n",
    "        df1, df2 = filter_value(df1, df2, col, 850, -1)\r\n",
    "    df1, df2 = process_category(df1, df2, col)\r\n",
    "    return df1, df2\r\n",
    "\r\n",
    "\r\n",
    "def feature(df1, df2):\r\n",
    "    def divided(x):\r\n",
    "        if x % 40 == 0:\r\n",
    "            return 2\r\n",
    "        elif not x:\r\n",
    "            return 1\r\n",
    "        else:\r\n",
    "            return 0\r\n",
    "\r\n",
    "    df1[\"160_height\"] = df1.dev_height.apply(divided)\r\n",
    "    df2[\"160_height\"] = df2.dev_height.apply(divided)\r\n",
    "    df1[\"160_width\"] = df1.dev_width.apply(divided)\r\n",
    "    df2[\"160_width\"] = df2.dev_width.apply(divided)\r\n",
    "    df1[\"160_ppi\"] = df1.final_ppi.apply(divided)\r\n",
    "    df2[\"160_ppi\"] = df2.final_ppi.apply(divided)\r\n",
    "    df1[\"hw_ratio\"] = df1.dev_height / df1.dev_width\r\n",
    "    df2[\"hw_ratio\"] = df2.dev_height / df2.dev_width\r\n",
    "    df1[\"hw_matrix\"] = df1.dev_height * df1.dev_width\r\n",
    "    df2[\"hw_matrix\"] = df2.dev_height * df2.dev_width\r\n",
    "    df1[\"inch\"] = (df1.dev_height ** 2 + df1.dev_width ** 2) ** 0.5 / df1.final_ppi\r\n",
    "    df2[\"inch\"] = (df2.dev_height ** 2 + df2.dev_width ** 2) ** 0.5 / df2.final_ppi\r\n",
    "    return df1, df2\r\n",
    "\r\n",
    "\r\n",
    "def rf_cast(df1, df2):\r\n",
    "    c1 = df1.dev_width.notnull()\r\n",
    "    c2 = df1.dev_height.notnull()\r\n",
    "    c3 = df1.dev_ppi.isna()\r\n",
    "    c4 = df1.dev_ppi.notnull()\r\n",
    "    df1[\"mynull1\"] = c1 & c2 & c3\r\n",
    "    df1[\"mynull2\"] = c1 & c2 & c4\r\n",
    "\r\n",
    "    predict = df1[\r\n",
    "        [\"apptype\", \"carrier\", \"dev_height\", \"dev_ppi\", \"dev_width\", \"media_id\", \"ntt\", \"mynull1\", \"mynull2\"]]\r\n",
    "\r\n",
    "    df_notnans = predict[predict.mynull2 == True]\r\n",
    "\r\n",
    "    # 75训练25预测\r\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\r\n",
    "        df_notnans[[\"apptype\", \"carrier\", \"dev_height\", \"dev_width\", \"media_id\", \"ntt\"]], df_notnans[\"dev_ppi\"],\r\n",
    "        train_size=0.75, random_state=6)\r\n",
    "    regr_multirf = RandomForestClassifier(n_estimators=100, max_depth=40, random_state=0, n_jobs=-1)\r\n",
    "    regr_multirf.fit(X_train, y_train)\r\n",
    "    score = regr_multirf.score(X_test, y_test)\r\n",
    "    print(\"prediction score is {:.2f}%\".format(score * 100))\r\n",
    "    df_nans = predict[predict.mynull1 == True].copy()\r\n",
    "    df_nans[\"dev_ppi_pred\"] = regr_multirf.predict(\r\n",
    "        df_nans[[\"apptype\", \"carrier\", \"dev_height\", \"dev_width\", \"media_id\", \"ntt\"]])\r\n",
    "    df1 = pd.merge(df1, df_nans[[\"dev_ppi_pred\"]], on=\"sid\", how=\"left\")\r\n",
    "    c1 = df2.dev_width.notnull()\r\n",
    "    c2 = df2.dev_height.notnull()\r\n",
    "    c3 = df2.dev_ppi.isna()\r\n",
    "    c4 = df2.dev_ppi.notnull()\r\n",
    "    df2[\"mynull1\"] = c1 & c2 & c3\r\n",
    "    df2[\"mynull2\"] = c1 & c2 & c4\r\n",
    "    predict_test = df2[\r\n",
    "        [\"apptype\", \"carrier\", \"dev_height\", \"dev_ppi\", \"dev_width\", \"media_id\", \"ntt\", \"mynull1\", \"mynull2\"]]\r\n",
    "    df_nans = predict_test[predict_test.mynull1 == True].copy()\r\n",
    "    df_nans[\"dev_ppi_pred\"] = regr_multirf.predict(\r\n",
    "        df_nans[[\"apptype\", \"carrier\", \"dev_height\", \"dev_width\", \"media_id\", \"ntt\"]])\r\n",
    "    df2 = pd.merge(df2, df_nans[[\"dev_ppi_pred\"]], on=\"sid\", how=\"left\")\r\n",
    "\r\n",
    "    def recol_ppi(df):\r\n",
    "        a = df.dev_ppi.fillna(0).values\r\n",
    "        b = df.dev_ppi_pred.fillna(0).values\r\n",
    "        c = []\r\n",
    "        # print(a,b)\r\n",
    "        for i in range(len(a)):\r\n",
    "            c.append(max(a[i], b[i]))\r\n",
    "        c = np.array(c)\r\n",
    "        df[\"final_ppi\"] = c\r\n",
    "        df[\"final_ppi\"][df[\"final_ppi\"] == 0] = None\r\n",
    "        return df\r\n",
    "\r\n",
    "    df1 = recol_ppi(df1)\r\n",
    "    df2 = recol_ppi(df2)\r\n",
    "    gc.collect()\r\n",
    "    return df1, df2\r\n",
    "\r\n",
    "\r\n",
    "def process_osv(df1, df2):\r\n",
    "    def process_osv1(x):\r\n",
    "        x = str(x)\r\n",
    "        if not x:\r\n",
    "            return -1\r\n",
    "        elif x.startswith(\"Android\"):\r\n",
    "            x = str(re.findall(\"\\d{1}\\.*\\d*\\.*\\d*\", x)[0])\r\n",
    "            return x\r\n",
    "        elif x.isdigit():\r\n",
    "            return x\r\n",
    "        else:\r\n",
    "            try:\r\n",
    "                x = str(re.findall(\"\\d{1}\\.\\d\\.*\\d*\", x)[0])\r\n",
    "                return x\r\n",
    "            except:\r\n",
    "                return 0\r\n",
    "\r\n",
    "    df1.osv = df1.osv.apply(process_osv1)\r\n",
    "    df2.osv = df2.osv.apply(process_osv1)\r\n",
    "    set3 = set(df1[\"osv\"].value_counts().head(70).index)\r\n",
    "\r\n",
    "    def process_osv2(x):\r\n",
    "        if x in set3:\r\n",
    "            return x\r\n",
    "        else:\r\n",
    "            return 0\r\n",
    "\r\n",
    "    df1[\"osv\"] = df1[\"osv\"].apply(process_osv2)\r\n",
    "    df2[\"osv\"] = df2[\"osv\"].apply(process_osv2)\r\n",
    "\r\n",
    "    le8 = preprocessing.LabelEncoder()\r\n",
    "    df1.osv = le8.fit_transform(df1.osv.astype(\"str\"))\r\n",
    "    df1[\"osv\"] = df1[\"osv\"].astype(\"object\")\r\n",
    "\r\n",
    "    df2.osv = le8.transform(df2.osv.astype(\"str\"))\r\n",
    "    df2[\"osv\"] = df2[\"osv\"].astype(\"object\")\r\n",
    "    return df1, df2\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# catboost 训练\r\n",
    "def catboost_training(train_path,test_path):\r\n",
    "    feature_train = pd.read_pickle(train_path)\r\n",
    "    feature_test = pd.read_pickle(test_path)\r\n",
    "    for col in [\"dev_height\", \"dev_width\", \"hw_ratio\", \"hw_matrix\", \"inch\", \"lan\"]:\r\n",
    "        if col in feature_train.columns:\r\n",
    "            feature_train[col] = feature_train[col].astype(\"float64\")\r\n",
    "            feature_test[col] = feature_test[col].astype(\"float64\")\r\n",
    "    cate_feature = ['apptype', 'carrier', 'media_id', 'os', 'osv', 'package', 'version', 'location', 'cus_type',\r\n",
    "                    \"fea1_hash\", \"fea_hash\", \"ntt\", \"os\", 'fea1_hash_ntt_combine', 'fea_hash_carrier_combine',\r\n",
    "                    'cus_type_osv_combine', 'fea1_hash_apptype_combine', 'fea_hash_media_id_combine',\r\n",
    "                    'cus_type_version_combine', 'apptype_ntt_combine', 'media_id_carrier_combine',\r\n",
    "                    'version_osv_combine', 'package_lan_combine', 'lan']\r\n",
    "\r\n",
    "    # x_col=df_importance.head(27).column\r\n",
    "\r\n",
    "    y_col = 'label'\r\n",
    "    x_col = ['apptype', 'carrier', 'dev_height',\r\n",
    "             'dev_width', 'lan', 'media_id', 'ntt', 'osv', 'package',\r\n",
    "             'timestamp', 'version', 'fea_hash', 'location', 'fea1_hash', 'cus_type',\r\n",
    "             'hour', 'minute',\r\n",
    "             '160_height',\r\n",
    "             'hw_ratio', 'hw_matrix', 'inch']\r\n",
    "    cate_feature = [x for x in cate_feature if x in x_col]\r\n",
    "    for item in cate_feature:\r\n",
    "        if item in ['fea1_hash_ntt_combine', 'fea_hash_carrier_combine', 'cus_type_osv_combine',\r\n",
    "                    'fea1_hash_apptype_combine', 'fea_hash_media_id_combine', 'cus_type_version_combine',\r\n",
    "                    'apptype_ntt_combine', 'media_id_carrier_combine', 'version_osv_combine', 'package_lan_combine']:\r\n",
    "            set4 = set(feature_train[item].value_counts().head(300).index)\r\n",
    "\r\n",
    "            def process_fea_hash(x):\r\n",
    "                if x in set4:\r\n",
    "                    return x\r\n",
    "                else:\r\n",
    "                    return -1\r\n",
    "\r\n",
    "            feature_train[item] = feature_train[item].apply(process_fea_hash).astype(\"str\")\r\n",
    "            feature_test[item] = feature_test[item].apply(process_fea_hash).astype(\"str\")\r\n",
    "        le = preprocessing.LabelEncoder()\r\n",
    "        feature_train[item] = le.fit_transform(feature_train[item])\r\n",
    "        feature_test[item] = le.transform(feature_test[item])\r\n",
    "\r\n",
    "    df_prediction = feature_test[x_col]\r\n",
    "    df_prediction['label'] = 0\r\n",
    "    model2 = CatBoostClassifier(loss_function=\"Logloss\",\r\n",
    "                                eval_metric=\"Accuracy\",\r\n",
    "                                learning_rate=0.03,\r\n",
    "                                iterations=10000,\r\n",
    "                                random_seed=42,\r\n",
    "                                od_type=\"Iter\",\r\n",
    "                                metric_period=10,\r\n",
    "                                depth=10,\r\n",
    "                                early_stopping_rounds=500,\r\n",
    "                                use_best_model=True,\r\n",
    "                                bagging_temperature=0.7,\r\n",
    "                                leaf_estimation_method=\"Newton\",\r\n",
    "                                )\r\n",
    "\r\n",
    "    li_f = []\r\n",
    "    df_importance_list = []\r\n",
    "    n = 10\r\n",
    "    # kfold = GroupKFold(n_splits=n)\r\n",
    "    kfold = KFold(n_splits=n, shuffle=True, random_state=220)\r\n",
    "    # weight = [0.1, 0.11, 0.1, 0.11, 0.11, 0.11, 0.05, 0.11, 0.1, 0.1]\r\n",
    "    # assert sum(weight) == 1 and len(weight) == n\r\n",
    "    for fold_id, (trn_idx, val_idx) in enumerate(kfold.split(feature_train[x_col], feature_train[y_col])):\r\n",
    "        X_train = feature_train.iloc[trn_idx][x_col]\r\n",
    "        Y_train = feature_train.iloc[trn_idx][y_col]\r\n",
    "\r\n",
    "        X_val = feature_train.iloc[val_idx][x_col]\r\n",
    "        Y_val = feature_train.iloc[val_idx][y_col]\r\n",
    "\r\n",
    "        print('\\nFold_{} Training ================================\\n'.format(fold_id + 1))\r\n",
    "        cat_model = model2.fit(\r\n",
    "            X_train,\r\n",
    "            Y_train,\r\n",
    "            cat_features=cate_feature,\r\n",
    "            # # eval_names=['train', 'valid'],\r\n",
    "            eval_set=(X_val, Y_val),\r\n",
    "            verbose=100,\r\n",
    "\r\n",
    "            # plot=True\r\n",
    "            # eval_metric=[\"auc\",\"binary_logloss\",\"binary_error\"],\r\n",
    "            # early_stopping_rounds=400\r\n",
    "        )\r\n",
    "\r\n",
    "        pred_val = cat_model.predict_proba(X_val, thread_count=-1)[:, 1]\r\n",
    "        df_oof = feature_train.iloc[val_idx].copy()\r\n",
    "        df_oof['pred'] = pred_val\r\n",
    "        li_f.append(df_oof)\r\n",
    "\r\n",
    "        pred_test = cat_model.predict_proba(feature_test[x_col], thread_count=-1)[:, 1]\r\n",
    "        df_prediction['label'] += pred_test / n\r\n",
    "        # prediction['label'] += pred_test\r\n",
    "\r\n",
    "        df_importance = pd.DataFrame({\r\n",
    "            'column': x_col,\r\n",
    "            'importance': cat_model.feature_importances_,\r\n",
    "        })\r\n",
    "        df_importance_list.append(df_importance)\r\n",
    "    return df_prediction, li_f, feature_train, feature_test\r\n",
    "\r\n",
    "\r\n",
    "def save(file_path, pred, df1, df2, threshold=0.5):\r\n",
    "    a = pd.DataFrame(pred.index)\r\n",
    "    a['label'] = pred[\"label\"].values\r\n",
    "\r\n",
    "    a.label = a.label.apply(lambda x: 1 if x > threshold else 0)\r\n",
    "    user_label = pd.DataFrame()\r\n",
    "\r\n",
    "    user_label[\"uid\"] = df1.android_id.values\r\n",
    "    user_label[\"ntt\"] = df1.ntt.values\r\n",
    "    temp = pd.DataFrame(df1.groupby([\"android_id\", \"ntt\"]).label.mean())\r\n",
    "    temp = temp.reset_index()\r\n",
    "    temp.rename(columns={\"android_id\": \"uid\", \"label\": \"label_prior\"}, inplace=True)\r\n",
    "    user_label = pd.merge(user_label, temp, on=[\"uid\", \"ntt\"], how=\"left\")\r\n",
    "    user_label.drop_duplicates(inplace=True)\r\n",
    "    a[\"uid\"] = df2.android_id.values\r\n",
    "    a[\"ntt\"] = df2.ntt.values\r\n",
    "    a = pd.merge(a, user_label, how=\"left\", on=[\"uid\", \"ntt\"])\r\n",
    "\r\n",
    "    def post(label, prior):\r\n",
    "        n = len(label)\r\n",
    "        count = 0\r\n",
    "        for i in range(n):\r\n",
    "            if 0 <= prior[i] <= 0.1 and label[i] == 1:\r\n",
    "                label[i] = 0\r\n",
    "                count += 1\r\n",
    "                # print(i)\r\n",
    "            elif 0.9 <= prior[i] <= 1 and label[i] == 0:\r\n",
    "                label[i] = 1\r\n",
    "                count += 1\r\n",
    "                # print(i)\r\n",
    "            else:\r\n",
    "                pass\r\n",
    "        print(count)\r\n",
    "        return label.values\r\n",
    "\r\n",
    "    a.label = post(a.label, a.label_prior)\r\n",
    "    a = a[[\"sid\", \"label\"]]\r\n",
    "    a.to_csv(file_path, index=False)\r\n",
    "    return a\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "carrier {0.0: 0, 46000.0: 1, 46001.0: 2, 46003.0: 3, -1.0: -1}\n",
      "prediction score is 97.39%\n",
      "\n",
      "Fold_1 Training ================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Custom logger is already specified. Specify more than one logger at same time is not thread safe.Warning: Overfitting detector is active, thus evaluation metric is calculated on every iteration. 'metric_period' is ignored for evaluation metric.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.8660156\ttest: 0.8676600\tbest: 0.8676600 (0)\ttotal: 679ms\tremaining: 1h 53m 11s\n",
      "100:\tlearn: 0.8847422\ttest: 0.8854600\tbest: 0.8854600 (100)\ttotal: 1m 18s\tremaining: 2h 7m 46s\n",
      "200:\tlearn: 0.8884067\ttest: 0.8884200\tbest: 0.8885200 (195)\ttotal: 2m 42s\tremaining: 2h 12m 9s\n"
     ]
    }
   ],
   "source": [
    "\r\n",
    "if __name__ == \"__main__\":\r\n",
    "    train = pd.read_csv('/home/aistudio/work/train.csv')\r\n",
    "    test1 = pd.read_csv('/home/aistudio/work/test1.csv')\r\n",
    "    df = data_preprocessing(train,test1)\r\n",
    "    df1 = df[0]\r\n",
    "    df2 = df[1]\r\n",
    "    for col in [\"location\", \"os\", \"ntt\", \"cus_type\"]:\r\n",
    "        df1, df2 = process_category(df1, df2, col)\r\n",
    "    for col, dict1 in zip([\"carrier\"], [{0.0: 0, 46000.0: 1, 46001.0: 2, 46003.0: 3, -1.0: -1}]):\r\n",
    "        df1, df2 = dict_category(df1, df2, col, dict1)\r\n",
    "    for col in [\"apptype\", \"media_id\", \"version\", \"lan\", \"package\", \"fea1_hash\", \"fea_hash\"]:\r\n",
    "        df1, df2 = special_category(df1, df2, col)\r\n",
    "    df1, df2 = process_osv(df1, df2)\r\n",
    "    df1, df2 = rf_cast(df1, df2)\r\n",
    "    df1, df2 = feature(df1, df2)\r\n",
    "    df1.to_pickle(\"/home/aistudio/work/processed_data/train.jlz\")\r\n",
    "    df2.to_pickle(\"/home/aistudio/work/processed_data/test.jlz\")\r\n",
    "    #输出文件名\r\n",
    "    filename = './sub.csv'\r\n",
    "    df_prediction, li_f, feature_train, feature_test  = catboost_training(\"/home/aistudio/work/processed_data/train.jlz\",\"/home/aistudio/work/processed_data/test.jlz\")\r\n",
    "    # latest = pd.concat(li_f)\r\n",
    "    # latest.to_pickle(\"./latest.pkl\")\r\n",
    "    save(filename, df_prediction, feature_train, feature_test)\r\n",
    "    print(\"Done!!!\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 2.1.0 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
